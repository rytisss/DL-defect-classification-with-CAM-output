{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vSHFTBu-1odD",
   "metadata": {
    "id": "vSHFTBu-1odD"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "x81T-y2VmDMR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x81T-y2VmDMR",
    "outputId": "6a1271df-68bd-4bb9-b5b6-247ccb23a1ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fa0b68",
   "metadata": {
    "id": "f4fa0b68"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Conv2D,\n",
    "    LeakyReLU, Activation, UpSampling2D, BatchNormalization, AveragePooling2D, concatenate)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41bd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to deal with files!\n",
    "\n",
    "def get_file_name_with_ext(path: str):\n",
    "    file_name_with_ext = os.path.basename(path)\n",
    "    return file_name_with_ext\n",
    "\n",
    "\n",
    "def get_file_name(path: str):\n",
    "    file_name_with_ext = os.path.basename(path)\n",
    "    file_name, file_extension = os.path.splitext(file_name_with_ext)\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def make_directory(output_dir: str):\n",
    "    if not os.path.exists(output_dir):\n",
    "        print('Making output directory: ' + output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "# get all image paths from directory\n",
    "def gather_image_from_dir(input_dir):\n",
    "    image_extensions = ['*.bmp', '*.jpg', '*.png']\n",
    "    image_list = []\n",
    "    for image_extension in image_extensions:\n",
    "        image_list.extend(glob.glob(input_dir + image_extension))\n",
    "    image_list.sort()\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7490e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history plotter\n",
    "\n",
    "def draw_training_history(history, number_of_epochs, saver, xbins=10):\n",
    "    # Plot\n",
    "    # training\n",
    "    epochs_ints = range(0, number_of_epoch)\n",
    "    epochs = [str(x) for x in epochs_ints]\n",
    "    \n",
    "    fig = plt.figure(figsize=(6.6, 4.8))\n",
    "    plt.plot(epochs, history.history['binary_accuracy'], 'b--')\n",
    "    plt.plot(epochs, history.history['val_binary_accuracy'], 'b')\n",
    "    plt.plot(epochs, history.history['precision'], 'g--')\n",
    "    plt.plot(epochs, history.history['val_precision'], 'g')\n",
    "    plt.plot(epochs, history.history['recall'], '--', color='orange')\n",
    "    plt.plot(epochs, history.history['val_recall'], color='orange')\n",
    "    plt.axvline(x=saver.best_epoch, color='r')\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "    plt.title('Model accuracy, recall and precision', fontsize=17)\n",
    "    plt.ylabel('accuracy / recall / precision')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_acc', 'val_acc', 'train_pre', 'val_pre', 'train_rec', 'val_rec', 'best_w'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.show()\n",
    "    \n",
    "    # print values\n",
    "    print('binary_accuracy')\n",
    "    print(history.history['binary_accuracy'])\n",
    "    print('val_binary_accuracy')\n",
    "    print(history.history['val_binary_accuracy'])\n",
    "    print('recall')\n",
    "    print(history.history['recall'])\n",
    "    print('val_recall')\n",
    "    print(history.history['val_recall'])\n",
    "    print('precision')\n",
    "    print(history.history['precision'])\n",
    "    print('val_precision')\n",
    "    print(history.history['val_precision'])\n",
    "\n",
    "    # testing\n",
    "    fig = plt.figure(figsize=(6.6, 4.8))\n",
    "    plt.plot(epochs, history.history['loss'], 'b--')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'g')\n",
    "    plt.axvline(x=saver.best_epoch, color='r')\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "    plt.title('Model loss', fontsize=17)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['loss', 'val_loss', 'best_w'], loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.show()\n",
    "    \n",
    "    # print values\n",
    "    print('loss')\n",
    "    print(history.history['loss'])\n",
    "    print('val_loss')\n",
    "    print(history.history['val_loss'])\n",
    "    \n",
    "    print(50 * '-')\n",
    "    print(f'Best Accuracy = {saver.best_acc_score}')\n",
    "    print(f'Best Recall = {saver.best_rec}')\n",
    "    print(f'Best Precision = {saver.best_precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for figure rendering plot\n",
    "\n",
    "def add_subplot(fig, rows, cols, pos, name, image, colorspace, min, max):\n",
    "    image_plot = fig.add_subplot(rows, cols, pos)\n",
    "    image_plot.title.set_text(name)\n",
    "    image_plot.title.set_fontsize(17)\n",
    "\n",
    "    if colorspace:\n",
    "        im = plt.imshow(image, cmap=colorspace, vmin=min, vmax=max)\n",
    "        divider = make_axes_locatable(image_plot)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        cb = plt.colorbar(im, cax=cax)\n",
    "    else:\n",
    "        im = plt.imshow(image)\n",
    "        \n",
    "def make_rgb(name, image, save_path):\n",
    "    fig = plt.figure(figsize=(6.6, 4.8))\n",
    "    norm_image = image / 255.\n",
    "    vmin = 0.0\n",
    "    vmax = 1.0\n",
    "    add_subplot(fig, 1, 1, 1, name, norm_image, None, vmin, vmax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def make_single_graph(name, image, save_path):\n",
    "    fig = plt.figure(figsize=(6.6, 4.8))\n",
    "    norm_image = image / 255.\n",
    "    colormap = 'jet'\n",
    "    vmin = 0.0\n",
    "    vmax = 1.0\n",
    "    add_subplot(fig, 1, 1, 1, name, norm_image, colormap, vmin, vmax)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def draw_fig(image, heatmap, name, output_path):\n",
    "    fig, ax = plt.subplots(figsize=(6.6, 4.8))\n",
    "    ax.imshow(image)\n",
    "    ax.imshow(heatmap, cmap='jet', alpha=0.32)\n",
    "    ax.set_title(name, fontsize=17)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc14848",
   "metadata": {
    "id": "5dc14848"
   },
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "# this code shared in https://stackabuse.com/learning-rate-warmup-with-cosine-decay-in-keras-and-tensorflow/\n",
    "def lr_warmup_cosine_decay(global_step,\n",
    "                           warmup_steps,\n",
    "                           hold = 0,\n",
    "                           total_steps=0,\n",
    "                           start_lr=0.0,\n",
    "                           target_lr=1e-3):\n",
    "    # Cosine decay\n",
    "    learning_rate = 0.5 * target_lr * (1 + np.cos(np.pi * (global_step - warmup_steps - hold) / float(total_steps - warmup_steps - hold)))\n",
    "\n",
    "    # Target LR * progress of warmup (=1 at the final warmup step)\n",
    "    warmup_lr = target_lr * (global_step / warmup_steps)\n",
    "\n",
    "    # Choose between `warmup_lr`, `target_lr` and `learning_rate` based on whether `global_step < warmup_steps` and we're still holding.\n",
    "    # i.e. warm up if we're still warming up and use cosine decayed lr otherwise\n",
    "    if hold > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold,\n",
    "                                 learning_rate, target_lr)\n",
    "    \n",
    "    learning_rate = np.where(global_step < warmup_steps, warmup_lr, learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "class WarmupCosineDecay(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_steps=0, warmup_steps=0, start_lr=0.0, target_lr=1e-3, hold=0):\n",
    "\n",
    "        super(WarmupCosineDecay, self).__init__()\n",
    "        self.start_lr = start_lr\n",
    "        self.hold = hold\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = 0\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = model.optimizer.lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = lr_warmup_cosine_decay(global_step=self.global_step,\n",
    "                                    total_steps=self.total_steps,\n",
    "                                    warmup_steps=self.warmup_steps,\n",
    "                                    start_lr=self.start_lr,\n",
    "                                    target_lr=self.target_lr,\n",
    "                                    hold=self.hold)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Ck389MdL3BMz",
   "metadata": {
    "id": "Ck389MdL3BMz"
   },
   "outputs": [],
   "source": [
    "# Custom weights saver (according to the best accuracy)\n",
    "\n",
    "class CustomSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, output_dir='', weights_name='best'):\n",
    "        self.best_acc_score = 0.0\n",
    "        self.best_rec = 0.0\n",
    "        self.best_precision = 0.0\n",
    "        self.best_epoch = 0\n",
    "        self.__output_dir = output_dir\n",
    "        # make output directory\n",
    "        if not os.path.exists(self.__output_dir):\n",
    "            os.makedirs(self.__output_dir)\n",
    "        self.__weights_name = weights_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # also save if validation error is smallest\n",
    "        if 'val_binary_accuracy' in logs.keys():\n",
    "            val_acc = logs['val_binary_accuracy']\n",
    "            if val_acc > self.best_acc_score:\n",
    "                self.best_acc_score = val_acc\n",
    "                self.best_rec = logs['val_recall']\n",
    "                self.best_precision = logs['val_precision']\n",
    "                self.best_epoch = epoch\n",
    "                output_path = self.__output_dir + '/' + self.__weights_name + '.hdf5'\n",
    "                print(f'New best weights found! Saving to {output_path}')\n",
    "                self.model.save(output_path)\n",
    "        else:\n",
    "            print('Key val_binary_accuracy does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bc4b60",
   "metadata": {
    "id": "24bc4b60"
   },
   "outputs": [],
   "source": [
    "# The following function makes the models mentioned in the article\n",
    "# 'last_stage_out' makes shortened version of the model with last convolutional stage out.\n",
    "# By shortening the model, we are getting bigger (wider) output with higher resolution for CAM!\n",
    "\n",
    "def effiecientNetV2B0Classifier(input_size=(128, 128, 3), last_stage_out=False, weights_path=None):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "    x = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_tensor=inputs)\n",
    "    if last_stage_out:\n",
    "        x = x.get_layer('block6a_expand_activation').output\n",
    "    else:\n",
    "        x = x.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "                  optimizer=Adam(learning_rate=1e-3),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                           tf.keras.metrics.Precision(thresholds=0.5),\n",
    "                           tf.keras.metrics.Recall(thresholds=0.5)])\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "def convnextTinyClassifier(input_size=(128, 128, 3), last_stage_out=False, weights_path=None):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "    x = tf.keras.applications.convnext.ConvNeXtTiny(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_tensor=inputs)\n",
    "    \n",
    "    if last_stage_out:\n",
    "        x = x.get_layer('convnext_tiny_stage_2_block_8_identity').output\n",
    "    else:\n",
    "        x = x.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "                  optimizer=Adam(learning_rate=1e-3),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                           tf.keras.metrics.Precision(thresholds=0.5),\n",
    "                           tf.keras.metrics.Recall(thresholds=0.5)])\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "def mobileNetV3Classifier(input_size=(128, 128, 3), last_stage_out=False, weights_path=None):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "    x = tf.keras.applications.MobileNetV3Large(include_top=False,\n",
    "                                                weights='imagenet',\n",
    "                                                input_tensor=inputs)\n",
    "    if last_stage_out:\n",
    "        x = x.layers[193].output\n",
    "    else:\n",
    "        x = x.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    model.compile(loss=tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "                  optimizer=Adam(learning_rate=1e-3),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5),\n",
    "                           tf.keras.metrics.Precision(thresholds=0.5),\n",
    "                           tf.keras.metrics.Recall(thresholds=0.5)])\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "xq5Hqy6xzNFq",
   "metadata": {
    "id": "xq5Hqy6xzNFq"
   },
   "outputs": [],
   "source": [
    "# Training sample\n",
    "\n",
    "# Input image information\n",
    "input_width = 752\n",
    "input_height = 480\n",
    "input_channels = 3\n",
    "# batch size. How many samples you want to feed in one iteration?\n",
    "batch_size = 2\n",
    "# number_of_epoch. How many epochs you want to train?\n",
    "number_of_epoch = 100\n",
    "# learning rate\n",
    "learning_rate=0.001\n",
    "# training percentage until full learning rate [0.0;1.0]\n",
    "warmup_percentage=0.15\n",
    "\n",
    "# weights output directory\n",
    "output_dir = 'Oliena_weights/efficientNet_4down/'\n",
    "# output weights name\n",
    "output_weights_name = 'efficientNet_4down'\n",
    "\n",
    "# data directories [in this sample we will just show how to use the code on few sample of the data]\n",
    "training_data_dir = 'data samples/'\n",
    "testing_data_dir = 'data samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "_ouoq_rH01T3",
   "metadata": {
    "id": "_ouoq_rH01T3"
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION\n",
    "# Augmentation pipeline\n",
    "def transform(image):\n",
    "    aug = A.Compose([\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.RandomGamma(p=0.2),\n",
    "        A.ShiftScaleRotate(p=0.2,\n",
    "                           shift_limit=[-0.1, 0.1],\n",
    "                           scale_limit=[-0.05,0.05],\n",
    "                           rotate_limit=[-5,5])\n",
    "    ])\n",
    "    return aug(image=image)['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e667b5a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e667b5a9",
    "outputId": "f2b5b5ae-12d3-4bb8-fb7c-e46cfaa66273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 images belonging to 2 classes.\n",
      "Found 8 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# data generators\n",
    "train_image_generator = ImageDataGenerator(preprocessing_function=transform)\n",
    "\n",
    "train_generator = train_image_generator.flow_from_directory(\n",
    "    training_data_dir,\n",
    "    target_size=(input_height, input_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary')\n",
    "\n",
    "test_image_generator = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_image_generator.flow_from_directory(\n",
    "    testing_data_dir, \n",
    "    target_size=(input_height, input_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saH4TMEJ1dXS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saH4TMEJ1dXS",
    "outputId": "5f3c6f7c-9ae4-493d-f8bd-9c592b301ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: 'adamw'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MODEL, chose from the provideded models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43meffiecientNetV2B0Classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_stage_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36meffiecientNetV2B0Classifier\u001b[0;34m(input_size, last_stage_out, weights_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, x)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinaryFocalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinaryAccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_path:\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights_path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/saving/legacy/serialization.py:368\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[1;32m    365\u001b[0m     class_name, custom_objects, module_objects\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown optimizer: 'adamw'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "# MODEL, chose from the provideded models\n",
    "model = effiecientNetV2B0Classifier(input_size=(input_height, input_width, input_channels), last_stage_out=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D2uAnOY61jC1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2uAnOY61jC1",
    "outputId": "c595ace9-792c-46ff-cec2-c0740f74b112"
   },
   "outputs": [],
   "source": [
    "# warmup and cosine schedule\n",
    "total_steps = train_generator.samples/batch_size*number_of_epoch\n",
    "# defined number of the steps\n",
    "warmup_steps = int(warmup_percentage*total_steps)\n",
    "\n",
    "print(f'Warmup steps: {warmup_steps}')\n",
    "\n",
    "schedule = WarmupCosineDecay(start_lr=0.0,\n",
    "                             target_lr=learning_rate,\n",
    "                             warmup_steps=warmup_steps,\n",
    "                             total_steps=total_steps,\n",
    "                             hold=warmup_steps)\n",
    "\n",
    "# custom saver serves as function to save the best performing weights\n",
    "saver = CustomSaver(output_dir=output_dir, weights_name=output_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b7ea35muv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f68b7ea35muv",
    "outputId": "8ccbbbc9-2452-4d80-c142-63fd9fed56ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples // batch_size,\n",
    "    validation_data = test_generator, \n",
    "    validation_steps = test_generator.samples // batch_size,\n",
    "    epochs = number_of_epoch,\n",
    "    callbacks=[schedule, saver],\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "model.load_weights(output_dir + output_weights_name + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c1681",
   "metadata": {
    "id": "989c1681"
   },
   "outputs": [],
   "source": [
    "x = model.layers[-3].output\n",
    "print(f'Output shape before average pooling: {x.shape}')\n",
    "weights = tf.expand_dims(model.layers[-1].get_weights()[0][:,0], 0)\n",
    "x = tf.keras.layers.dot([x, weights], axes=(3, 1))\n",
    "print(f'Output shape of expanded weights: {weights.shape}')\n",
    "print(f'Output after multiplication: {x.shape}')\n",
    "# make multihead output\n",
    "multi_head_model = tf.keras.Model(inputs=model.input, \n",
    "        outputs=(x, model.layers[-1].output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9e51d",
   "metadata": {
    "id": "8fd9e51d"
   },
   "outputs": [],
   "source": [
    "# Lets plot some DEFECT CAMS!\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # let matplotlib save more (>370) images\n",
    "\n",
    "image_output = output_dir + 'images/'\n",
    "make_directory(image_output)\n",
    "\n",
    "cam_output = output_dir + 'cam/'\n",
    "make_directory(cam_output)\n",
    "\n",
    "# take the defect directory in the test set\n",
    "image_paths = gather_image_from_dir(testing_data_dir + '1/')\n",
    "print(f'Found {len(image_paths)} images')\n",
    "# lets predict\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR) # no need to switch channels, it is actually greyscale\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (model.input.shape[2], model.input.shape[1])) # no need if cropped\n",
    "    image_name = get_file_name(image_path)\n",
    "    \n",
    "    # prepocess image\n",
    "    # preprocess\n",
    "    image_norm = image\n",
    "    image_norm = np.reshape(image_norm, image_norm.shape + (1,))\n",
    "    image_norm = np.reshape(image_norm, (1,) + image_norm.shape)\n",
    "    \n",
    "    # Get the output of last convolutional layer and prediction\n",
    "    last_conv_output, prediction = multi_head_model.predict(image_norm)\n",
    "    last_conv_output = last_conv_output[0,:,:]# we have only one image, so take the first one\n",
    "    last_conv_output_scaled = cv2.resize(last_conv_output, (model.input.shape[2], model.input.shape[1]))\n",
    "    \n",
    "    draw_fig(image, last_conv_output_scaled, 'image+defectCAM', cam_output + image_name + '_defectCAM_superpos.png')\n",
    "    \n",
    "    print(f'{image_name} {last_conv_output_scaled.shape} {prediction}')\n",
    "    make_rgb('image', image, image_output + image_name + '_rgb.png')\n",
    "print(f'Finish! Check {image_output} and {cam_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f54edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
